{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Test all the different functionalities that were implemented\n",
    "\n",
    "### already tested (state 29.08.2020)\n",
    "- linear_regression class\n",
    "- clustering (still some bugs in function 'get_dendrogram_data')\n",
    "- dimension reduction (LDA + PCA)\n",
    "- Gaussian Mixture Models with Expectation Maximization Algorithm\n",
    "- Gaussian Processes\n",
    "- Reinforcement Learning (Action-Value-iteration and Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# import numpy as np\n",
    "# import library.classic_ml\n",
    "# reload(library.classic_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from library.classic_ml import linear_regression\n",
    "# lr = linear_regression()\n",
    "\n",
    "# x = [-3, -1, 0, 2, 4]\n",
    "# y = [-4, 1, 1, 2, 6]\n",
    "# x_test = x\n",
    "\n",
    "# x2 = [[1,2],[2,1]]\n",
    "# y2 = [[2,2],[3,4]]\n",
    "# x2_test = [[2,1]]\n",
    "\n",
    "# lr.train(x,y,mode=\"l\")\n",
    "# lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from library.classic_ml import clustering\n",
    "# c = clustering()\n",
    "\n",
    "# data = np.array([[2,2.5],[7,0.5],[12,1],[4,-0.5],[10,2],[3,2],[7,0],[9,1.5],[4,2.5],[10,1]])\n",
    "# x_test = np.array([[1,5]])\n",
    "# c.train(data, mode=\"nubs\")\n",
    "# c.predict(x_test)\n",
    "# dd_data = c.get_dendrogram_data(c.cluster_centers, c.labels,\"average\")\n",
    "# print(dd_data)\n",
    "# c.make_dendrogram(dd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from library.classic_ml import dimension_reduction\n",
    "# d = dimension_reduction()\n",
    "\n",
    "# class1 = np.array([[4,1],[2,4],[2,3],[3,6],[4,4]])\n",
    "# class2 = np.array([[9,10],[6,8],[9,5],[8,7],[10,8]])\n",
    "# test = np.array([7,4])\n",
    "# d.pca(np.array(class1),1,1)\n",
    "# d.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from library.classic_ml import gmm\n",
    "# g = gmm()\n",
    "\n",
    "# xs1 = [1,3,4]\n",
    "# xs = [[0,1],[0.5,1],[4,6],[5,7],[8,7]]\n",
    "# xs_test = [7,4]\n",
    "# xs1_test = [5]\n",
    "# g.train(xs)\n",
    "# l1 = g.predict(xs_test)\n",
    "# g.train(xs1)\n",
    "# l2 = g.predict(xs1_test)\n",
    "# l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from library.classic_ml import gp\n",
    "# gp = gp()\n",
    "\n",
    "# x = np.array([1,1.2,1.4])\n",
    "# x_3 = np.array([1.4,2])\n",
    "# x_t = np.array([1.6])\n",
    "\n",
    "# y = np.array([1.5,1,0.8])\n",
    "# y_test, cov = gp.train(x,x,y,0.1,0.3,\"rbf\",True)\n",
    "# y_test, cov, y_test.shape, cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from importlib import reload\n",
    "import library.reinforcement_learning\n",
    "reload(library.reinforcement_learning)\n",
    "from library.reinforcement_learning import reinforcement_learning\n",
    "rl = reinforcement_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# states = np.array([1, 2, 3, 4])\n",
    "# rewards = np.array([1, -1, -1, 10])\n",
    "# gamma = 0.9\n",
    "# actions = np.array([-1,1])\n",
    "# probabilities = np.array([0.8,0.2])\n",
    "# probabilities = np.sort(probabilities)[::-1]\n",
    "# terminal_states = np.array([1, 4])\n",
    "# tolerance = 0.1\n",
    "# rl.action_value_iteration(states, actions, rewards, terminal_states, probabilities, gamma, tolerance, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# states = np.array([[1, 2, 3, 4],[5, \"x\", 7, 8],[9, 10, 11, 12]])\n",
    "# rewards = np.array([[0, 0, 0, 0],[0, 0, 0, -10],[0, 0, 0, 1]])\n",
    "# gamma = 0.9\n",
    "# alpha = (1/3)\n",
    "# actions = np.array([-1,0,1])\n",
    "# probabilities = np.array([0.1,0.1,0.8])\n",
    "# probabilities = np.sort(probabilities)[::-1]\n",
    "# terminal_states = np.array([(2,3), (1,3)])\n",
    "\n",
    "# tolerance = 0.1\n",
    "# rl.action_value_iteration(states, actions, rewards, terminal_states, probabilities, gamma, tolerance, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# states = np.array([1, 2, 3, 4])\n",
    "# rewards = np.array([1, -1, -1, 10])\n",
    "# gamma = 0.9\n",
    "# alpha = (1/3)\n",
    "# actions = np.array([-1,1])\n",
    "# probabilities = np.array([0.8,0.2])\n",
    "# probabilities = np.sort(probabilities)\n",
    "# terminal_states = np.array([0, 3])\n",
    "# steps = 10\n",
    "\n",
    "# probs = np.random.choice(actions.astype(str),steps,p=probabilities)\n",
    "# directions = np.random.choice([\"R\",\"L\"],steps,p=[1/len(actions) for _ in actions])\n",
    "# sequence = np.array([\"\".join(el) for el in np.vstack((directions, probs)).T])\n",
    "# transition  = {\"R1\": 1, \"R-1\": -1, \"L1\": -1, \"L-1\": 1}\n",
    "# rl.q_learning(states, actions, rewards, terminal_states, sequence, transition, gamma, alpha, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# states = np.array([[1, 2, 3, 4],[5, \"x\", 7, 8],[9, 10, 11, 12]])\n",
    "# rewards = np.array([[0, 0, 0, 0],[0, 0, 0, -10],[0, 0, 0, 1]])\n",
    "# gamma = 0.9\n",
    "# alpha = (1/3)\n",
    "# actions = np.array([-1,0,1])\n",
    "# probabilities = np.array([0.1,0.1,0.8])\n",
    "# terminal_states = np.array([(2,3), (1,3)])\n",
    "\n",
    "# steps = 1000\n",
    "\n",
    "# probs = np.random.choice(actions.astype(str),steps,p=probabilities)\n",
    "# directions = np.random.choice([\"R\",\"S\",\"L\"],steps,p=[1/len(actions) for _ in actions])\n",
    "# sequence = np.array([\"\".join(el) for el in np.vstack((directions, probs)).T])\n",
    "# transition  = {\"R1\": 1, \"R0\": 0, \"R-1\": -1, \"S1\": 0, \"S0\": 1, \"S-1\": -1, \"L1\": -1, \"L0\": 0, \"L-1\": 1}\n",
    "# rl.q_learning(states, actions, rewards, terminal_states, sequence, transition, gamma, alpha, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
